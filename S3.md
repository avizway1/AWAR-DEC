### **S3: Simple Storage Service**

---

#### **Object-Based Storage**
- **Definition:** Designed for storing flat files like documents, images, videos, and backups. Applications and operating systems cannot be installed or run on S3.  
  **Examples:** Dropbox, Google Drive.

---

#### **Other Storage Types**
1. **Block-Based Storage:**
   - Designed for running operating systems and applications.
   - Examples: EBS, Instance Store Volumes (used with EC2 instances).
2. **File-Based Storage:**
   - Network-based storage mounted to multiple devices.
   - Examples: EFS, FSx (used with EC2 instances).

---

#### **Key Features of S3**
- **Object Storage:** Ideal for storing flat files like logs, images, and videos.
- **Data Organization:** Data is stored in **buckets**.
- **Global Uniqueness:** Bucket names must be unique across all AWS accounts globally.
- **Soft Limit:** By default, an account can create up to 10000 buckets.
- **Scalability:** Unlimited data can be stored within a bucket.
- **No Pre-Provisioning:** Automatically scales as data is added.
  
---

#### **Bucket Naming Rules**
1. **Length:** Minimum 3 characters, maximum 63 characters.
2. **Character Restrictions:**
   - Must use lowercase letters and numbers.
   - Cannot start or end with a period (`.`).
   - Adjacent periods (`..`) are not allowed.
   - Must not resemble an IP address (e.g., `192.168.100.1`).

---

#### **Free Tier Eligibility**
- **5 GB** Standard Storage.
- **2,000 PUT** (uploads) / **20,000 GET** (downloads).

---

#### **Object Storage Details**
- **Object Size:** Minimum size: **0 bytes**, Maximum size: **5 TB**.
- **Region-Specific:** While S3 is a global service, the data resides in the AWS Region's Availability Zones (AZs) where the bucket is created.

#### **Example URLs:**
1. **Path-Style URL (Legacy):** `https://s3.ap-south-1.amazonaws.com/bucketname/objectname`
2. **Virtual-Hosted URL:** 
   - `https://bucketname.s3.ap-south-1.amazonaws.com/objectname`
   - `https://bucketname.s3.amazonaws.com/objectname`
3. **Note:** Virtual paths won't work for bucket names containing periods (`.`).

---

#### **Making an Object Public**
1. **Account-Level Blocking:**
   - Ensure "Block Public Access" is disabled at the account level.
2. **Bucket-Level Blocking:**
   - Disable "Block Public Access" for the specific bucket.
3. **Object-Level Setting:**
   - Use ACLs to make the object public:
     - Navigate to the object → Actions → Make public using ACL.

---

### **We have total 3 types of buckets.**
- general Purpose bucket
- Directory Bucket
- Table bucket
---

### **1. General Purpose Buckets**
- **Description:**  
  General Purpose Buckets are the original S3 bucket type, suitable for most use cases and access patterns. They support a wide range of storage classes, except for specific single-zone storage classes like **S3 Express One Zone** and **S3 One Zone-IA** in AWS Local Zones.

- **Primary Use Cases:**  
  - Storing frequently or infrequently accessed data across multiple Availability Zones (AZs).  
  - General-purpose storage for web applications, media hosting, or data backups.  
  - Use cases requiring high durability and availability across multiple AZs.

- **Supported Storage Classes:**  
  - **S3 Standard**  
  - **S3 Standard-IA**  
  - **S3 Glacier Flexible Retrieval**  
  - **S3 Glacier Deep Archive**  

---

### **2. Directory Buckets**
- **Description:**  
  Directory Buckets are designed for specific use cases requiring low-latency or data residency, allowing bucket creation in a specific location such as an Availability Zone or a Local Zone.  
  - **Availability Zone (AZ):** Supports the **S3 Express One Zone** storage class for low-latency, high-performance workloads.  
  - **Local Zone:** Supports the **S3 One Zone-IA (Z-IA)** storage class for data residency requirements.

- **Primary Use Cases:**  
  - **High-Performance Workloads:**  
    - For latency-sensitive applications requiring single-digit millisecond PUT and GET operations in an AZ.  
    - Example: Analytics or real-time processing.  
  - **Data Residency Workloads:**  
    - Storing data in a specific AWS Local Zone to meet residency or compliance requirements.  

- **Supported Storage Classes:**  
  - **S3 Express One Zone** (for Availability Zone workloads).  
  - **S3 One Zone-IA** (for Local Zone workloads).  

---

### **3. Table Buckets**
- **Description:**  
  Table Buckets are used to store structured or tabular data, often serialized in formats like JSON, CSV, Parquet, or Avro. These buckets are optimized for use in data lakes and analytics workflows.

- **Primary Use Cases:**  
  - **Data Analytics:**  
    - Storing structured data for querying using tools like Amazon Athena, AWS Glue, or Redshift Spectrum.  
  - **Data Lakes:**  
    - Serving as a central repository for data that can be processed and analyzed.  
  - **Database Export/Import:**  
    - Exporting and importing tabular data to and from relational or NoSQL databases.  

- **Common File Formats:**  
  - JSON, CSV, Parquet, Avro.  

---

### **Comparison Table**

| **Feature**                        | **General Purpose Buckets**                  | **Directory Buckets**                             | **Table Buckets**                                |
|------------------------------------|---------------------------------------------|--------------------------------------------------|-------------------------------------------------|
| **Scope**                          | Multi-AZ                                     | Single AZ or Local Zone                          | Multi-AZ (typically).                           |
| **Latency**                        | Standard                                     | Low latency (single-digit millisecond).          | Optimized for query workloads (varies by tools).|
| **Use Case**                       | General-purpose storage for diverse data.    | Performance-sensitive or residency workloads.    | Data lakes, analytics, and structured storage.  |
| **Supported Storage Classes**      | S3 Standard, Standard-IA, Glacier classes.   | S3 Express One Zone, S3 One Zone-IA.            | S3 Standard, Intelligent Tiering, Glacier.      |

---

<img width="1220" alt="Screenshot 2024-12-27 at 9 03 08 AM" src="https://github.com/user-attachments/assets/da56e22b-79ba-4b15-a58a-ef2cc379c054" />



### **S3 Commonly used Storage Classes Use Cases**
1. **S3 Standard (Default):**  
   - **Use Case:** Frequently accessed data.  
   - **Availability:** 99.99%, **Durability:** 99.999999999% (11 nines).  
   - **Replication:** Data stored across **≥3 AZs**.

2. **S3 Standard-IA (Infrequent Access):**  
   - **Use Case:** Infrequently accessed data.  
   - **Availability:** 99.9%, **Durability:** 99.999999999%.  
   - **Replication:** Data stored across **≥3 AZs**.

3. **S3 One Zone-IA:**  
   - **Use Case:** Infrequently accessed non-critical data.  
   - **Availability:** 99.5%, **Durability:** 99.999999999%.  
   - **Replication:** Data stored in **1 AZ**.

4. **S3 Glacier Flexible Retrieval:**  
   - **Use Case:** Long-term archival storage (retrieval in minutes to hours).  
   - **Availability:** 99.99%, **Durability:** 99.999999999%.  
   - **Replication:** Data stored across **≥3 AZs**.  

5. **S3 Glacier Deep Archive:**  
   - **Use Case:** Long-term archival with 12+ hour retrieval times.  
   - **Replication:** Data stored across **≥3 AZs**.

6. **S3 Intelligent-Tiering:**  
   - **Use Case:** Optimized cost for unknown access patterns.  
   - **Availability:** 99.9%, **Durability:** 99.999999999%.  
   - **Replication:** Data stored across **≥3 AZs**.

---

#### **Data Retrieval Types (Glacier)**
- **Bulk:** 5–12 hours.  
- **Standard:** 3–5 hours.  
- **Expedited:** 1–5 minutes (up to 250 MB).

---

#### **Task : Create an S3 bucket. Upload and object, make it public. Verify object access using Object URL.**
**Try to understand aws s3 storage classes. (Certification exam topic)
---

#### **S3 Pricing Factors**
1. Data stored in buckets.  
2. Number of PUT (upload) and GET (download) requests.  
3. Data retrieval and transfer costs.


## **S3 Versioning**

### **Overview**
- S3 Versioning allows you to maintain multiple versions of an object within the same bucket.
- By default, versioning is in a **Suspended** state for any bucket.
- Once versioning is **enabled**, S3 will keep track of all versions of objects in the bucket.

### **Delete Behavior**
1. **Typing "Delete" when deleting an object:**  
   - S3 creates a **Delete Marker**, making the object invisible in the current version view.
   - You can delete the Delete Marker to recover the object.

2. **Typing "Permanently Delete" when deleting an object:**  
   - The object is permanently removed, and no Delete Marker is created.

### **Key Scenarios**
1. **Versioning Enabled:**
   - **HIDE Mode (Default View):**
     - Only the most recent version (current version) is visible.
     - Deleting an object adds a Delete Marker.
     - To recover the object, switch to "SHOW Versions," locate the Delete Marker, and remove it.
   - **SHOW Mode (Version View):**
     - All versions, including older ones and Delete Markers, are visible.
     - Deleting an object in this mode removes it permanently.

2. **Versioning Suspended:**
   - No version tracking occurs.
   - Deleting an object will permanently remove it.

---

## **Lifecycle Management Rules**

Lifecycle Management Rules help automate object transitions and deletions to optimize storage costs.

### **Scope of Rules**
- **Entire Bucket:** Apply rules to all objects in the bucket.
- **Prefix-Based Rules:** Apply rules to specific folders or object paths within the bucket.
- **Tag-Based Rules:** Apply rules only to objects with specific tags.

### **Sample Scenarios**
1. **S3 Standard → Standard-IA → Glacier → Delete**  
   Objects transition through Standard-IA (Infrequent Access) to Glacier and are eventually deleted.

2. **S3 Standard → Glacier → Delete**  
   Objects transition directly to Glacier and are deleted after a retention period.

3. **S3 Standard → Delete**  
   Objects remain in Standard storage before being deleted without transitioning to lower-cost tiers.


![lifecycle-transitions-v3](https://github.com/user-attachments/assets/4c597e4f-a1b3-49c7-970b-e558e92af2d1)


---

## **Replication Rules**

### **Cross-Region Replication (CRR) / Same-Region Replication (SRR)**
Replication ensures objects are copied from a source bucket to a destination bucket.

- **Requirements:**
  - Both source and destination buckets must have versioning enabled.
  - Existing objects are **not replicated**; only future uploads are replicated.
  - To replicate all existing objects, AWS offers **AWS Batch** job to replicate. **Cost US**

- **Replication Time Control (RTC):**
  - Guarantees data replication to the destination bucket within **15 minutes** for 99.99% of the data.
  - Additional costs apply for RTC.

### **Cost Considerations**
- **Data Transfer Between Regions:** Charges apply for data transfer.
- **Data Transfer Within a Region:** No cost for replication within the same region.

### **Manual Replication Using AWS CLI**
For one-time or scheduled replication, use the AWS CLI `sync` command:

```bash
aws s3 sync s3://source-bucket s3://target-bucket
```
- Schedule the command using:
  - **Windows Task Scheduler**
  - **Linux Cron Jobs** (e.g., daily replication at 12:00 AM).

---

## **Summary Table**

| **Feature**                        | **Behavior**                                                                                     |
|------------------------------------|--------------------------------------------------------------------------------------------------|
| **Versioning Default State**       | Suspended                                                                                       |
| **Delete Behavior (Versioning ON)**| Creates a Delete Marker; delete the marker to recover the object.                               |
| **Delete Behavior (Versioning OFF)**| Permanently deletes the object.                                                                |
| **Lifecycle Rules Scope**          | Entire bucket, prefix-based, or tag-based rules.                                                |
| **Replication Requirements**       | Source and destination buckets must have versioning enabled.                                    |
| **Replication Time Control**       | Guarantees replication within 15 minutes for 99.99% of data (additional cost).                  |
| **Manual Replication**             | Use AWS CLI `sync` command for scheduled or ad-hoc replication.                                 |

---


## **Event Notifications**

### **When to Use S3 Event Notifications**  
S3 event notifications are helpful when you want to automate tasks or trigger actions in response to certain changes in your S3 bucket. Some common use cases include:  
- **Data Ingestion:** Notify downstream systems whenever a new file is uploaded to the bucket.  
- **Data Processing:** Trigger a Lambda function for image processing, transcoding videos, or extracting metadata from uploaded files.  
- **Monitoring and Alerting:** Use SNS to send alerts for object changes.  
- **Workflow Automation:** Integrate with SQS for queue-based workflows where other systems consume messages.

### **Available Destinations for Event Notifications**  
1. **SNS (Simple Notification Service):**  
   - Enables notifications to subscribers via email, SMS, or HTTP.  
   - **Free Tier:** 1,000 emails per month.  

2. **SQS (Simple Queue Service):**  
   - Reliable and scalable message queuing service.  
   - Suitable for decoupled systems where multiple consumers process events asynchronously.  

3. **Lambda:**  
   - Executes code in response to events.  
   - Ideal for on-the-fly transformations or actions without managing infrastructure.

### **Steps to Configure S3 Event Notifications**  
1. **Create or Identify the Destination:**  
   - For SNS: Create an SNS topic and add subscribers.  
   - For SQS: Set up a queue.  
   - For Lambda: Create the Lambda function.

2. **Modify Permissions:**  
   - **SNS-S3 Integration:**  
     - Update the SNS topic's **Access Policy** to allow S3 to publish messages.  
     - Example policy to allow everyone:  
       ```json
       {
         "Effect": "Allow",
         "Principal": "*",
         "Action": "SNS:Publish",
         "Resource": "arn:aws:sns:<region>:<account-id>:<topic-name>"
       }
       ```
   - Ensure your bucket policy allows events to be sent to the specified destination.

3. **Set Up the Notification on the S3 Bucket:**  
   - Go to the **Properties** tab of the bucket.  
   - Configure event notifications for specific events like **PUT**, **POST**, **DELETE**, or **Restore Object**.

4. **Test the Setup:**  
   - Perform an action on the bucket (e.g., upload a file).  
   - Verify that the event triggers the destination (e.g., email alert, queue message, or Lambda execution).

---

## **Using KMS with S3 for Encryption**

### **What is Encryption?**  
Encryption is the process of converting data into a secure format to prevent unauthorized access. Only authorized users with the correct decryption key can access the original data.

### **Importance of S3 Data Encryption**
- **Data Protection:** Ensures sensitive data is secure both during transmission and at rest.  
- **Regulatory Compliance:** Helps meet compliance requirements like GDPR, HIPAA, or CCPA.  
- **Access Control:** Provides additional layers of security to control who can access or decrypt data.

---

### **Types of Encryption in S3**

#### **1. In-Transit Encryption:**  
- Protects data while being transmitted to or from S3.  
- **Technology Used:**  
  - **SSL (Secure Socket Layer)**  
  - **TLS (Transport Layer Security)**  

#### **2. Server-Side Encryption (SSE) (At Rest):**  
- Protects data stored on the S3 platform.  
- **Options:**  

1. **SSE-S3:**  
   - S3 manages encryption keys (AES-256).  
   - Suitable for public data.  
   - No direct access to key material by users.

2. **SSE-KMS (Key Management Service):**  
   - **AWS Managed Keys (aws/s3):**  
     - Default encryption keys created and managed by AWS KMS.  
     - Cannot be deleted.  
   - **Customer Managed Keys (CMK):**  
     - User-created keys.  
     - Requires permissions for both the S3 bucket and the encryption key.  
     - **Note:** Data encrypted with KMS keys cannot be made public.  

3. **SSE-C (Customer Provided Keys):**  
   - Customer provides key material to AWS.  
   - AWS uses it to encrypt and decrypt data.  
   - Customers can rotate or revoke keys anytime.  

#### **3. Client-Side Encryption (CSE):**  
- Encrypt data using your own tools before uploading it to S3.  
- AWS is not responsible for managing or decrypting the data.

---

### **Steps to Create a KMS Key for S3 Encryption**
1. **Choose Key Type:**  
   - Symmetric (S3 supports only symmetric keys).  
2. **Set Key Administrators:**  
   - Define who can manage the key.  
3. **Assign Key Users:**  
   - Grant access to users or services that will encrypt/decrypt data.  
4. **Review and Create the Key:**  
   - Confirm the configuration and create the key.

---

## **S3 Static Website Hosting**

### **Setup for Static Website Hosting**
1. **Bucket Configuration:**  
   - Create an S3 bucket with the same name as your domain (e.g., `learnaws.co.in`).  
   - Upload your static website files (HTML, CSS, JS).  

2. **Permissions:**  
   - Update the bucket policy to allow public read access.  

3. **Enable Static Website Hosting:**  
   - Go to the bucket’s **Properties** tab.  
   - Enable **Static Website Hosting** and set the index and error document (e.g., `index.html`, `error.html`).

4. **Access Your Website:**  
   - Use the static website endpoint, e.g.,  
     `http://learnaws.co.in.s3-website.ap-south-1.amazonaws.com`

### **Common HTTP Status Codes**
- **2XX:** Success (e.g., 200 OK).  
- **3XX:** Redirection (e.g., 301 Moved Permanently).  
- **4XX:** Client Errors (e.g., 403 Forbidden, 404 Not Found).  
- **5XX:** Server Errors (e.g., 500 Internal Server Error).

---


## **Bucket Policies Overview**

### **When to Use Bucket Policies?**  
Bucket policies are used to define permissions at the bucket level for access control and operations. These policies allow you to:  
- **Control Access:** Manage permissions for individual IAM users, groups, roles, or other AWS accounts.  
- **Enforce Security:** Specify who can access the bucket and what actions are allowed or denied.  
- **Implement Restrictions:** Apply conditions, such as IP address restrictions, to enhance security.

### **Common Scenarios for Using Bucket Policies**  
1. **Restrict Public Access:** Prevent unauthorized public access to sensitive data.  
2. **Cross-Account Access:** Grant access to users from another AWS account.  
3. **Deny Specific Actions:** For example, deny users from deleting or overwriting objects.  
4. **Restrict IP Address Access:** Allow or deny bucket access based on source IP addresses.  
5. **Log Delivery:** Enable write permissions for AWS services like CloudTrail or S3 access logs.

---

### **Example Scenario: Deny Object Uploads to a Specific Bucket**

**Objective:** Deny the `PutObject` operation for a specific user on a bucket.  

#### **Key Details**  
- **Bucket ARN:** `arn:aws:s3:::avinash.bucket`  
- **IAM User ARN (Principal):** `arn:aws:iam::501170964283:user/s3user`  
- **Effect:** Deny  
- **Actions to Deny:** `s3:PutObject`, `s3:DeleteBucket`

#### **Bucket Policy Example**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Principal": {
        "AWS": "arn:aws:iam::501170964283:user/s3user"
      },
      "Action": [
        "s3:PutObject",
        "s3:DeleteBucket"
      ],
      "Resource": [
        "arn:aws:s3:::avinash.bucket",
        "arn:aws:s3:::avinash.bucket/*"
      ]
    }
  ]
}
```

### **Understanding the Resources**
- **Bucket Level Operations:** Use `arn:aws:s3:::avinash.bucket` for actions that apply to the bucket itself (e.g., deleting the bucket).  
- **Object-Level Operations:** Use `arn:aws:s3:::avinash.bucket/*` for actions that apply to objects inside the bucket (e.g., uploading or deleting objects).  
- **For General Use:** Combine both, as shown above, when unsure whether the operation is at the bucket or object level.

---

## **S3 Object Lock**

### **What is Object Lock?**
S3 Object Lock is a feature that helps you prevent objects from being deleted or overwritten. This is particularly useful for ensuring compliance and protecting data against accidental or malicious deletion.

### **Prerequisites**
- **Versioning:** Must be enabled on the bucket before configuring Object Lock.

### **Modes of Object Lock**
1. **Governance Mode:**  
   - Allows users with specific IAM permissions to delete or overwrite locked objects during the retention period.  
   - Suitable for scenarios where administrative overrides are required.

2. **Compliance Mode:**  
   - Prevents all users, including root users, from deleting or overwriting objects during the retention period.  
   - Ideal for regulatory compliance or highly secure environments.

---

### **Steps to Enable Object Lock**
1. **Create a Bucket with Object Lock Enabled:**  
   - In the S3 Management Console, check the "Enable Object Lock" option during bucket creation.  

2. **Set Retention Configuration:**  
   - Navigate to the **Properties** tab of the bucket.  
   - Under **Object Lock**, configure retention settings (retention period, legal hold, etc.).  

3. **Test the Configuration:**  
   - Upload an object to the bucket.  
   - Verify that the object cannot be deleted or overwritten according to the mode (Governance or Compliance).

---

## **S3 Performance Overview**

### **Default Performance of an S3 Bucket**  
- **Write Operations:** Up to **3,500 PUT requests per second per prefix**.  
- **Read Operations:** Up to **5,500 GET requests per second per prefix**.  

### **How to Increase S3 Bucket Performance?**

1. **Increase Object Name Uniqueness:**  
   - Use random prefixes or date-based structures in object names to distribute workload across partitions.  
   - Example: Instead of `logs/file1.txt`, use `logs/2024/12/30/file1.txt`.

2. **Use More Prefixes:**  
   - Divide objects into multiple prefixes (folders) to spread requests.  
   - Example: Instead of storing all logs in `logs/`, use `logs/region1/` and `logs/region2/`.

---

## **Server Access Logging**

### **What is Server Access Logging?**  
Server access logging provides detailed records about the requests made to an S3 bucket, including the source of the request, the action performed, and the response. This is crucial for auditing and monitoring access to your data.

### **Reference:**  
[Amazon S3 Server Access Log Format](https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html)

---

## **CloudTrail Data Events**

### **What are CloudTrail Data Events?**  
CloudTrail Data Events provide detailed insights into API activity at the data level for S3 buckets. It tracks actions such as `GetObject`, `PutObject`, and `DeleteObject`.  

### **Benefits**
- Logs activities for **current and future buckets**.  
- Provides enhanced security and auditing capabilities.

---

## **Requester Pays**

### **What is Requester Pays?**  
With the **Requester Pays** feature enabled, the person requesting the data pays for both the data transfer and request costs instead of the bucket owner.  

### **Key Points**
- **Anonymous access is disabled** automatically when this feature is enabled.  
- Ideal for public datasets where data consumers cover the costs.

---

## **Transfer Acceleration**

### **What is Transfer Acceleration?**  
Transfer Acceleration uses **AWS Edge Locations** to accelerate uploads and downloads to and from S3. It is particularly useful for users located far from the bucket’s AWS Region.  

### **Key Points**
- **Additional Cost:** Transfer Acceleration incurs extra charges.  
- Not supported for buckets with **periods (.)** in their names.

### **How to Test Transfer Acceleration?**
AWS provides a [speed comparison tool](https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html?region=ap-south-1&origBucketName=testbucket) to evaluate the performance benefits.

---

## **Storage Class Analysis**

### **What is Storage Class Analysis?**  
Storage Class Analysis helps you monitor and analyze storage access patterns to identify objects that can be transitioned to cheaper storage classes, such as S3 Standard-IA or S3 Glacier.

---

## **Cross-Origin Resource Sharing (CORS)**

### **What is CORS?**
CORS defines rules for sharing resources between different domains. For example, if a web application hosted on `example.com` needs to access images from an S3 bucket in `anotherdomain.com`, you need to configure CORS.

### **Common Use Cases**
- Allowing client-side scripts from a web application to access objects in S3.  
- Enabling dynamic image loading from S3 for websites.

### **Steps to Configure CORS on S3**
1. Open the **S3 Console** and select your bucket.  
2. Go to the **Permissions** tab.  
3. Scroll to **CORS Configuration** and click **Edit**.  
4. Add a JSON configuration, such as:
   ```json
   [
     {
       "AllowedHeaders": ["*"],
       "AllowedMethods": ["GET", "POST", "PUT"],
       "AllowedOrigins": ["https://example.com"],
       "ExposeHeaders": ["x-amz-server-side-encryption"],
       "MaxAgeSeconds": 3000
     }
   ]
   ```
5. Save the changes.

---

## **S3 Consistency Model**

Amazon S3 provides a **highly durable and available object storage service**, and it follows specific **consistency models** to ensure data integrity and availability. These consistency models are key to understanding how updates, reads, and deletions work in S3.

### Consistency Models for Amazon S3

1. **Strong Read-After-Write Consistency**
   - **Definition**: When a new object is created, any read request for that object (immediately after a write) will return the newly written data.
   - **Applicability**: Applies to new object PUT operations.
   - **Example**: If you upload a new image to S3 and immediately attempt to access it, you will always get the latest version.

2. **Strong Consistency for Overwrite PUT and DELETE Operations**
   - **Definition**: Amazon S3 ensures strong consistency for overwrite and delete operations, meaning:
     - If an existing object is overwritten with new data, subsequent read requests return the latest data.
     - If an object is deleted, subsequent read requests return a "404 Not Found" response.
   - **Example**: If you replace a file named `report.pdf` with a new version, any read operation will return the updated file. Similarly, if you delete the file, it will no longer be accessible.

3. **Consistency for List Operations**
   - **Definition**: Amazon S3 offers **eventual consistency** for listing objects in a bucket. This means changes like newly added or deleted objects might not immediately reflect in the list results.
   - **Example**: If you upload or delete an object and then list the objects in the bucket, it might take a short time for the list results to update.

---

### Summary of S3 Consistency Models

| Operation Type           | Consistency Model           |
|---------------------------|-----------------------------|
| **New Object Upload**     | Strong Consistency          |
| **Overwrite PUT**         | Strong Consistency          |
| **DELETE**                | Strong Consistency          |
| **GET**                   | Strong Consistency          |
| **LIST**                  | Eventual Consistency        |

---


---

## **Multipart Upload for Large Files**

### **When to Use Multipart Upload?**
- Recommended for files larger than **100 MB** and mandatory for files larger than **5 GB**.  
- If you upload large objects over a stable high-bandwidth network, use multipart upload to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance.
- If you upload over a spotty network, use multipart upload to increase resiliency against network errors by avoiding upload restarts. When using multipart upload, you only need to retry uploading the parts that are interrupted during the upload. You don't need to restart uploading your object from the beginning.

### **Steps for Multipart Upload via CLI**
1. Use the AWS CLI to initiate the upload:
   ```bash
   aws s3 cp largefile.zip s3://your-bucket-name/ --part-size 10MB
   ```

---

## **AWS Snow Family**

### **Overview**
The Snow Family provides offline data transfer devices for moving large datasets to AWS.  

- **AWS Snowcone:** 8 TB capacity, portable and rugged.  
- **AWS Snowball:** 100 TB capacity, suited for bulk data migration.  
- **AWS Snowball Edge:** 40-100 TB capacity, supports edge computing.  
- **AWS Snowmobile:** Exabyte-scale transfer using a shipping container.

### **Example:**  
For 1 PB of data over a 1 Gbps internet line:  
- Transfer time = **20-25 years**  
- Snow Family is ideal in such cases.

---



Comparison chart for **Regular Internet**, **Direct Connect**, **Site-to-Site VPN**, and **AWS Snow Family**:

| Feature                 | **Regular Internet**               | **AWS Direct Connect**             | **Site-to-Site VPN**               | **AWS Snow Family**                 |
|-------------------------|-------------------------------------|-------------------------------------|-------------------------------------|-------------------------------------|
| **Connection Type**     | Standard internet connection       | Dedicated physical connection       | Encrypted connection over the internet | Physical device for offline transfer |
| **Latency**             | High and inconsistent              | Low and consistent                  | Higher and variable                | No latency (offline data transfer)  |
| **Bandwidth**           | Limited by ISP                     | High bandwidth options available (1 Gbps, 10 Gbps, etc.) | Dependent on the internet connection | Varies (Snowcone: 8 TB, Snowball: 100 TB, Snowmobile: 1 EB) |
| **Security**            | Minimal encryption, exposed to public internet | High, as it bypasses the public internet | Secure with IPsec encryption       | Highly secure physical transfer     |
| **Cost**                | Standard ISP charges               | Higher due to dedicated line        | Lower compared to Direct Connect   | Device rental and shipping costs    |
| **Setup Time**          | Instant setup                      | 3-4 weeks                          | Few hours                          | Days to weeks (depending on shipping) |
| **Use Case**            | General-purpose browsing, small data transfers | Stable, high-performance connections for production workloads | Secure and quick temporary connectivity | Bulk data migration to AWS          |
| **Availability**        | Varies by ISP                      | Global availability via AWS partners | Available globally                 | Limited by shipping regions         |
| **Example Scenarios**   | Uploading files via browser or CLI | Running hybrid workloads, extending on-prem to AWS | Quick disaster recovery or test environments | Migrating PB-scale data to AWS       |
| **Suitability for Large Data Transfers** | Poor                | Excellent                          | Moderate                           | Best for large datasets (> 100 TB) |
| **Scalability**         | Limited                            | Scalable with additional ports or connections | Limited to the internet bandwidth | Scales with multiple devices (Snowball/Snowmobile) |

